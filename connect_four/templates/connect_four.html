{% extends "layout.html" %}

{% block title %}Connect Four - Juega contra IA con Aprendizaje por Refuerzo{% endblock %}

{% block head_styles %}
<link rel="stylesheet" href="{{ url_for('connect_four.static', filename='css/connect_four.css') }}">
<!-- CSS del tema (puedes elegir otros temas en highlightjs.org) -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/atom-one-dark.min.css">
<!-- highlight.js -->
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>

{% endblock %}

{% block content %}
<h2 class="text-white fall-element fall-delay-1">Connect Four - Juega contra IA con Reinforcement Learning</h2>
<p class="lead text-white fall-element fall-delay-1">¡Intenta conectar 4 fichas en línea antes que la IA entrenada con Deep Q-Learning!</p>

<!-- Selector de dificultad -->
<div class="difficulty-selector fall-element fall-delay-2">
    <label for="difficultySelector" class="text-white mb-2">Nivel de dificultad:</label>
    <select id="difficultySelector" class="form-select mb-3" style="max-width: 200px;">
        <option value="normal" selected>Normal</option>
        <option value="hard">Avanzado</option>
    </select>
    <p class="text-white-50 small">
        Normal: El modelo RL con 30% de aleatoriedad.
        Avanzado: El modelo RL puro.
    </p>
</div>

<div class="row fall-element fall-delay-2">
    <div class="col-lg-8 col-md-12 mb-4">
        <!-- Contenedor del tablero -->
        <div class="board-container">
            <!-- Mensaje de ganador -->
            <div class="winner-message" id="winnerMessage"></div>

            <!-- Selectores de columna - AHORA ENCIMA DEL TABLERO -->
            <div class="mb-2">
                <div id="columnSelectors" class="d-flex gap-2 justify-content-center">
                    <div class="column-selector" data-column="0"></div>
                    <div class="column-selector" data-column="1"></div>
                    <div class="column-selector" data-column="2"></div>
                    <div class="column-selector" data-column="3"></div>
                    <div class="column-selector" data-column="4"></div>
                    <div class="column-selector" data-column="5"></div>
                    <div class="column-selector" data-column="6"></div>
                </div>
            </div>

            <!-- Tablero de juego -->
            <div class="board" id="gameBoard">
                <!-- Las celdas se generarán con JavaScript -->
            </div>
        </div>

        <!-- Botones de juego -->
        <div class="game-buttons fall-element fall-delay-3">
            <button id="resetButton" class="btn btn-primary">Reiniciar Juego</button>
            <button id="aiFirstButton" class="btn btn-secondary">IA Primero</button>
        </div>
    </div>

    <div class="col-lg-4 col-md-12">
        <!-- Información del juego - AHORA AL LADO DEL TABLERO -->
        <div class="game-info fall-element fall-delay-3">
            <h4>Estado del Juego</h4>
            <div class="turn-indicator player-turn" id="turnIndicator">Tu turno</div>

            <div class="game-stats text-dark">
                <p><strong>Fichas jugadas:</strong> <span id="moveCount">0</span></p>
                <p><strong>Tiempo de juego:</strong> <span id="gameTime">00:00</span></p>
            </div>
        </div>
    </div>
</div>

<!-- Explicación del modelo -->
<div class="jumbotron fall-element fall-delay-4">
    <h4>Acerca del Modelo de Reinforcement Learning</h4>
    <p>Este juego utiliza una Red Neuronal entrenada con <strong>Deep Q-Learning</strong>, un algoritmo de aprendizaje por refuerzo que permite al agente aprender a tomar decisiones jugando contra sí mismo.</p>

    <div class="row mt-3">
        <div class="col-md-6">
            <h5>¿Qué es el Reinforcement Learning?</h5>
            <p>El aprendizaje por refuerzo es un tipo de aprendizaje automático donde un agente aprende a tomar decisiones óptimas a través de la interacción con un entorno, recibiendo recompensas o castigos según los resultados de sus acciones.</p>
            <p>A diferencia del modelo CNN anterior (entrenado con datos de partidas), este modelo ha aprendido por experiencia propia, descubriendo estrategias por sí mismo.</p>
        </div>
        <div class="col-md-6">
            <h5>Arquitectura DQN</h5>
            <ul>
                <li><strong>Deep Q-Network:</strong> El modelo utiliza redes convolucionales para procesar el tablero como una imagen 6x7 y predecir el valor Q (calidad) de cada posible acción.</li>
                <li><strong>Entrenamiento:</strong> El modelo se entrenó durante 20,000 episodios, mejorando continuamente a través de su propia experiencia.</li>
                <li><strong>Recompensas:</strong> +10 por ganar, +1 por empate, -5 por perder, y -10 por intentar un movimiento inválido.</li>
            </ul>
        </div>
    </div>


    <button type="button" class="code-toggle mt-3 btn btn-secondary mb-3 fall-element fall-delay-2"
        data-bs-toggle="collapse" data-bs-target="#modelCode">
        <i class="bi bi-code-slash"></i> Ver código de entrenamiento RL
    </button>



    <div class="collapse" id="modelCode">
        <pre><code class="language-python">
import tensorflow as tf
import numpy as np
from collections import deque
import random

# Parámetros del entrenamiento RL
GAMMA = 0.99              # Factor de descuento
MEMORY_SIZE = 50000       # Tamaño del buffer de experiencias
BATCH_SIZE = 64           # Tamaño del batch para entrenamiento
EPSILON_START = 1.0       # Epsilon inicial (exploración)
EPSILON_MIN = 0.1         # Epsilon mínimo
EPSILON_DECAY = 0.9995    # Tasa de decaimiento de epsilon
LEARNING_RATE = 0.0001    # Tasa de aprendizaje

# Crear el modelo Deep Q-Network
model = tf.keras.Sequential([
    # Reshape para convertir a 6x7x1
    tf.keras.layers.Reshape((6, 7, 1), input_shape=(6, 7)),
    
    # Capas convolucionales
    tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
    tf.keras.layers.BatchNormalization(),
    
    # Capas densas
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(7)  # Q-valores para cada acción (columna)
])

# Entrenamiento principal con estrategia epsilon-greedy
for episode in range(EPISODES):
    state = env.reset()
    done = False
    
    while not done:
        # Seleccionar acción (exploración vs explotación)
        if random.random() < epsilon:
            action = random.choice(env.valid_moves)  # Exploración
        else:
            q_values = model.predict(state)[0]  # Explotación
            action = np.argmax(q_values)
            
        # Ejecutar acción en el entorno
        next_state, reward, done, info = env.step(action)
        
        # Almacenar experiencia y entrenar la red
        replay_buffer.add(state, action, reward, next_state, done)
        state = next_state
        
        # Actualizar red con lotes de experiencias pasadas
        batch = replay_buffer.sample(BATCH_SIZE)
        # [Cálculo de objetivos Q y actualización de pesos]
        
    # Reducir epsilon para favorecer más explotación
    epsilon = max(EPSILON_MIN, epsilon * EPSILON_DECAY)
        </code></pre>
    </div>
</div>

{% endblock %}

{% block scripts %}
<script src="{{ url_for('connect_four.static', filename='js/connect_four.js') }}"></script>
{% endblock %}